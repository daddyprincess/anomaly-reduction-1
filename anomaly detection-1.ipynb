{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7446c919-ac2d-4279-95d5-bcb6a7e97426",
   "metadata": {},
   "source": [
    "## Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143f059-9974-416e-9ed8-9bc05fe993c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomaly detection, also known as outlier detection, is a technique used in data analysis and machine learning to\n",
    "identify patterns or data points that deviate significantly from the expected or normal behavior in a dataset. The\n",
    "purpose of anomaly detection is to flag or detect unusual, rare, or abnormal instances within a given dataset, which\n",
    "may indicate potential problems, fraud, errors, or interesting insights.\n",
    "\n",
    "Here are some key points about anomaly detection:\n",
    "\n",
    "1.Identification of Unusual Patterns: Anomaly detection algorithms aim to identify data points or patterns that differ\n",
    "significantly from the majority of the data. These deviations are often referred to as anomalies, outliers, or\n",
    "anomalies.\n",
    "\n",
    "2.Applications: Anomaly detection is used in various fields, including cybersecurity (detecting network intrusions),\n",
    "fraud detection (identifying unusual financial transactions), manufacturing (finding defective products), healthcare\n",
    "(spotting unusual patient symptoms), and more.\n",
    "\n",
    "3.Types of Anomalies: Anomalies can be categorized into different types, such as point anomalies (individual data \n",
    "points that are outliers), contextual anomalies (data points that are anomalies in a specific context but not in\n",
    "others), and collective anomalies (groups of data points that are abnormal when considered together).\n",
    "\n",
    "4.Techniques: There are various techniques and algorithms for anomaly detection, including statistical methods,\n",
    "machine learning approaches (such as clustering, classification, and autoencoders), and domain-specific rule-based\n",
    "methods.\n",
    "\n",
    "5.Unsupervised Learning: Many anomaly detection methods are based on unsupervised learning because anomalies are\n",
    "often rare and may not have labeled examples. Unsupervised methods aim to learn the normal behavior from the data\n",
    "and flag deviations.\n",
    "\n",
    "6.Evaluation: The effectiveness of an anomaly detection system is typically evaluated using metrics like precision,\n",
    "recall, F1-score, and receiver operating characteristic (ROC) curves, depending on the specific application.\n",
    "\n",
    "Overall, anomaly detection is crucial for identifying unexpected and potentially harmful events or data points in\n",
    "various domains, helping organizations take timely actions to mitigate risks or improve their processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b342e-d68e-4fa3-bec1-92d5a57a063a",
   "metadata": {},
   "source": [
    "## Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50c82e-9c78-4f95-8544-69032346eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomaly detection is a valuable technique, but it comes with several key challenges that practitioners must address\n",
    "to build effective anomaly detection systems. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "1.Scarcity of Anomalies: Anomalies are often rare events in a dataset, making them challenging to detect, especially\n",
    "in large and imbalanced datasets. The scarcity of anomalies can lead to imprecise models and high false-positive rates.\n",
    "\n",
    "2.Labeling Anomalies: In many real-world applications, anomalies may not be explicitly labeled in the training data,\n",
    "making it difficult to use supervised learning approaches. This leads to a reliance on unsupervised or semi-supervised \n",
    "methods, which can be less accurate.\n",
    "\n",
    "3.Data Quality: Anomaly detection relies on the assumption that the training data accurately represents the normal\n",
    "behavior. Noisy or incomplete data can hinder the model's ability to learn and generalize from the data effectively.\n",
    "\n",
    "4.Feature Engineering: Selecting relevant features or variables for anomaly detection is crucial. In some cases,\n",
    "feature engineering can be challenging, especially when dealing with high-dimensional data or unstructured data types\n",
    "like text or images.\n",
    "\n",
    "5.Data Imbalance: Anomalies are typically a minority class in a dataset, leading to class imbalance. Imbalanced \n",
    "datasets can bias models towards the majority class and make it harder to detect anomalies.\n",
    "\n",
    "6.Dynamic Environments: Anomaly detection models may need to adapt to changing environments where the definition of \n",
    "normal behavior evolves over time. Static models may become less effective as the data distribution shifts.\n",
    "\n",
    "7.Interpretable Results: Interpreting the reasons behind detected anomalies can be challenging, especially in complex\n",
    "models like deep learning-based approaches. Understanding why a particular data point is flagged as an anomaly is\n",
    "crucial for taking appropriate actions.\n",
    "\n",
    "8.Choosing the Right Algorithm: Selecting the most suitable anomaly detection algorithm for a specific problem can be \n",
    "a challenge. There's no one-size-fits-all solution, and the choice often depends on the data characteristics and \n",
    "application domain.\n",
    "\n",
    "9.Scalability: Processing and analyzing large-scale datasets for anomaly detection can be computationally intensive. \n",
    "Scalability and efficiency become important considerations when dealing with big data.\n",
    "\n",
    "10.False Positives: Striking a balance between detecting true anomalies and minimizing false positives is a constant \n",
    "challenge. Aggressive anomaly detection may result in a high false-positive rate, while overly conservative approaches\n",
    "may miss important anomalies.\n",
    "\n",
    "11.Evaluation Metrics: Determining the appropriate evaluation metrics for an anomaly detection system can be tricky. \n",
    "Traditional classification metrics may not be suitable due to the class imbalance, requiring the use of specialized\n",
    "metrics like precision-recall curves.\n",
    "\n",
    "12.Concept Drift: In dynamic environments, the concept of normal behavior may change over time due to various factors. \n",
    "Anomaly detection models should be able to adapt to concept drift to maintain their effectiveness.\n",
    "\n",
    "Addressing these challenges often requires a combination of domain expertise, careful data preprocessing, feature \n",
    "engineering, model selection, and ongoing monitoring and adaptation of anomaly detection systems to maintain their\n",
    "accuracy and relevance in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de982a72-8603-4e71-bb24-2e64ae164be5",
   "metadata": {},
   "source": [
    "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9a262-f796-4efd-946f-e1c390a7590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies\n",
    "or outliers in a dataset. They differ primarily in terms of the availability of labeled data during the training\n",
    "process and the nature of the learning process. Here's how they differ:\n",
    "\n",
    "1.Supervised Anomaly Detection:\n",
    "\n",
    "    ~Labeled Data: In supervised anomaly detection, you have a dataset in which anomalies or outliers are explicitly \n",
    "    labeled. This means that you have examples of both normal and anomalous instances in your training data.\n",
    "\n",
    "    ~Learning Process: The learning process involves training a machine learning model, such as a classifier (e.g.,\n",
    "    decision tree, support vector machine, neural network), using the labeled data. The model learns to distinguish\n",
    "    between normal and anomalous instances based on the provided labels.\n",
    "\n",
    "    ~Use Case: Supervised anomaly detection is suitable when you have a reasonably large labeled dataset of anomalies\n",
    "    and normal instances, making it possible to build a model that can accurately classify new, unseen data points as\n",
    "    either normal or anomalous.\n",
    "\n",
    "    ~Pros:\n",
    "\n",
    "        ~High accuracy if labeled data is representative.\n",
    "        ~Explicit classification of anomalies.\n",
    "        \n",
    "    ~Cons:\n",
    "\n",
    "        ~Requires labeled data, which may be costly or unavailable.\n",
    "        ~May not perform well when anomalies are rare and hard to label.\n",
    "        \n",
    "2.Unsupervised Anomaly Detection:\n",
    "\n",
    "    ~Labeled Data: Unsupervised anomaly detection operates without labeled data for anomalies. Instead, it relies\n",
    "    solely on the characteristics of the data itself to identify anomalies.\n",
    "\n",
    "    ~Learning Process: Unsupervised methods aim to learn the natural patterns or distribution of the data. Anything\n",
    "    that significantly deviates from this learned pattern is considered an anomaly. Common techniques include \n",
    "    clustering (e.g., k-means), density estimation (e.g., Gaussian Mixture Models), and autoencoders.\n",
    "\n",
    "    ~Use Case: Unsupervised anomaly detection is useful when labeled anomaly data is scarce or unavailable. It can\n",
    "    discover anomalies based on deviations from the majority behavior in the absence of explicit labels.\n",
    "\n",
    "    ~Pros:\n",
    "\n",
    "        ~Doesn't require labeled anomaly data.\n",
    "        ~Can identify novel, previously unseen anomalies.\n",
    "        ~Suitable for scenarios where anomalies are rare or evolving.\n",
    "        \n",
    "    ~Cons:\n",
    "\n",
    "        ~May produce false positives if the learned normal behavior is not representative.\n",
    "        ~Difficulty in interpreting results and understanding why specific instances are flagged as anomalies.\n",
    "        \n",
    "In summary, the key difference between unsupervised and supervised anomaly detection lies in the availability of\n",
    "labeled data. Supervised methods rely on labeled anomalies and normal data to train a classification model, while\n",
    "unsupervised methods learn patterns from the data itself to detect anomalies without explicit labels. The choice\n",
    "between these approaches depends on the availability of labeled data, the nature of the problem, and the desired\n",
    "level of interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b509169-8a69-4bc2-b08a-51e42f7ef9aa",
   "metadata": {},
   "source": [
    "## Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3235f35-3ae3-4af0-9979-8aad338bdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomaly detection algorithms can be categorized into several main categories, each with its own approach to identifying\n",
    "anomalies in data. These categories include:\n",
    "\n",
    "1.Statistical Methods:\n",
    "\n",
    "    ~Z-Score (Standard Score): Measures how many standard deviations a data point is from the mean.\n",
    "    ~Modified Z-Score: A variation of the Z-Score that is robust to outliers.\n",
    "    ~Percentiles/Quantiles: Identifies anomalies based on values outside specified percentiles.\n",
    "    ~Grubbs' Test: Detects outliers in univariate data using the maximum deviation from the mean.\n",
    "    ~Mahalanobis Distance: Measures the distance of a data point from the center of the data distribution,\n",
    "    considering correlations.\n",
    "    \n",
    "2.Machine Learning-Based Methods:\n",
    "\n",
    "    ~Clustering: Unsupervised clustering algorithms like k-means can identify data points that do not belong to any\n",
    "    cluster or belong to small clusters as anomalies.\n",
    "    ~Classification: Supervised classification algorithms, such as support vector machines (SVMs), decision trees,\n",
    "    and random forests, can be used to classify data points as normal or anomalous when labeled data is available.\n",
    "    ~Autoencoders: Neural networks trained to learn a compressed representation of the data can be used to detect\n",
    "    anomalies by reconstructing input data and measuring reconstruction error.\n",
    "    ~Isolation Forest: Uses decision trees to isolate anomalies by randomly partitioning the data space.\n",
    "    ~One-Class SVM: Learns a boundary that encapsulates the normal data, identifying data points outside this boundary\n",
    "    as anomalies.\n",
    "    ~Local Outlier Factor (LOF): Measures the local density deviation of a data point with respect to its neighbors\n",
    "    to detect local anomalies.\n",
    "    \n",
    "3.Density-Based Methods:\n",
    "\n",
    "    ~Kernel Density Estimation (KDE): Estimates the probability density function of the data and flags data points\n",
    "    with low probability as anomalies.\n",
    "    ~Gaussian Mixture Models (GMM): Models data as a mixture of Gaussian distributions and identifies anomalies based\n",
    "    on low likelihood under the model.\n",
    "    \n",
    "4.Time-Series Anomaly Detection:\n",
    "\n",
    "    ~Seasonal Decomposition of Time Series (STL): Separates time series data into seasonal, trend, and residual\n",
    "    components and flags anomalies in the residual component.\n",
    "    ~ARIMA and Exponential Smoothing: Traditional time-series forecasting methods can detect anomalies by identifying\n",
    "    significant deviations from predicted values.\n",
    "    ~Prophet: A forecasting model designed for business time series data, capable of identifying outliers.\n",
    "    \n",
    "5.Ensemble Methods:\n",
    "\n",
    "    ~Combining Multiple Algorithms: Ensemble methods, such as voting, stacking, or boosting, combine the results of\n",
    "    multiple anomaly detection algorithms to improve overall performance and robustness.\n",
    "    \n",
    "6.Deep Learning-Based Methods:\n",
    "\n",
    "    ~Deep Autoencoders: Deep neural networks with multiple hidden layers can capture complex patterns in high-\n",
    "    dimensional data for anomaly detection.\n",
    "    ~Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM): These are used for time-series anomaly\n",
    "    detection by modeling sequential dependencies in data.\n",
    "    \n",
    "7.Domain-Specific Methods:\n",
    "\n",
    "    ~Some fields, such as network security, fraud detection, and industrial quality control, have specialized anomaly\n",
    "    detection methods tailored to their specific data and requirements.\n",
    "    \n",
    "8.Graph-Based Methods:\n",
    "\n",
    "    ~In scenarios where data has a graph structure (e.g., social networks, fraud detection networks), graph-based\n",
    "    algorithms can identify anomalous nodes or edges based on connectivity and properties of the graph.\n",
    "    \n",
    "The choice of an anomaly detection algorithm depends on factors like the nature of the data, the availability of \n",
    "labeled data, the desired interpretability, and the specific problem domain. It's common to experiment with multiple\n",
    "algorithms to determine which one works best for a given use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c00189-ad66-4601-b713-d928ef4b7a08",
   "metadata": {},
   "source": [
    "## Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e345c-ee16-4ad9-9ef8-7031b258de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Distance-based anomaly detection methods rely on specific assumptions about the data and the nature of anomalies.\n",
    "These assumptions guide the algorithms in identifying anomalies based on the distances between data points. The main\n",
    "assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "1.Distance-Based Separation: Distance-based methods assume that anomalies are significantly distant from normal data\n",
    "points in the feature space. In other words, anomalies are expected to have larger distances or dissimilarities\n",
    "compared to normal instances.\n",
    "\n",
    "2.Euclidean Distance: Many distance-based methods, such as the k-Nearest Neighbors (k-NN) and density-based methods\n",
    "like LOF (Local Outlier Factor), use Euclidean distance as a measure of dissimilarity. This assumes that the data is\n",
    "well-behaved and that Euclidean distance accurately reflects data similarity.\n",
    "\n",
    "3.Constant Density: Some distance-based methods assume that the density of normal data points is approximately\n",
    "constant within a certain neighborhood. Anomalies are expected to have a significantly lower local density, making\n",
    "them stand out.\n",
    "\n",
    "4.Isolation Principle: The isolation forest algorithm assumes that anomalies can be isolated more quickly than normal\n",
    "instances when constructing decision trees. This assumption is based on the idea that anomalies are fewer in number \n",
    "and exhibit higher variability.\n",
    "\n",
    "5.Unimodal Data Distribution: Distance-based methods often assume that the data follows a unimodal distribution, such\n",
    "as a Gaussian distribution, where normal instances cluster around the mean, and anomalies are located in the tails or\n",
    "far from the mean.\n",
    "\n",
    "6.Independence of Features: Distance-based methods typically assume that features are independent or weakly correlated.\n",
    "In cases where features are highly correlated, Euclidean distance may not accurately capture the true dissimilarity\n",
    "between data points.\n",
    "\n",
    "7.Symmetry of Distances: Distance-based methods assume that the distance metric used is symmetric, meaning that the\n",
    "distance from point A to point B is the same as the distance from point B to point A. While this is generally true\n",
    "for Euclidean distance, it may not hold in all situations (e.g., when using customized distance metrics).\n",
    "\n",
    "8.Homogeneity of Density: Some distance-based methods assume that the density of data points within a cluster or \n",
    "neighborhood is relatively homogeneous. Anomalies are expected to have lower-density regions around them.\n",
    "\n",
    "It's important to note that these assumptions may not always hold in real-world datasets, and the effectiveness of\n",
    "distance-based anomaly detection methods can be affected by violations of these assumptions. Therefore, careful \n",
    "consideration of the data characteristics and potential deviations from these assumptions is necessary when applying \n",
    "distance-based techniques. Additionally, choosing an appropriate distance metric and tuning parameters is essential\n",
    "for improving the performance of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00383dcf-91d6-4ea8-97b7-944123527cc0",
   "metadata": {},
   "source": [
    "## Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da5d65-2271-445c-91c9-da164a370b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Local Outlier Factor (LOF) algorithm is a density-based anomaly detection method that computes anomaly scores for\n",
    "data points based on their local deviation from the surrounding data points. LOF quantifies how much more or less\n",
    "dense a data point is compared to its neighbors. Here's a step-by-step explanation of how LOF computes anomaly scores:\n",
    "\n",
    "1.Define Parameters:\n",
    "\n",
    "    ~k: The user defines the number of nearest neighbors (typically denoted as \"k\") to consider when assessing the\n",
    "    local density of a data point.\n",
    "    ~Dataset: LOF operates on a dataset containing data points for which we want to detect anomalies.\n",
    "    \n",
    "2.Compute k-Nearest Neighbors (k-NN):\n",
    "    \n",
    "    ~For each data point in the dataset, LOF computes its k-NN, which are the k data points that are closest to the\n",
    "    point in terms of a distance metric (often Euclidean distance). These neighbors represent the local context of\n",
    "    the data point.\n",
    "\n",
    "3.Compute Reachability Distance:\n",
    "    ~For each data point, LOF computes the reachability distance of that point with respect to its k-NN. The \n",
    "    reachability distance of point A from point B is the maximum of the distance between A and B and the distance \n",
    "    of B's k-th nearest neighbor from itself. The reachability distance measures how far point A is from its k-NN,\n",
    "    considering the most distant neighbor.\n",
    "\n",
    "4.Compute Local Reachability Density:\n",
    "    \n",
    "    ~For each data point, LOF calculates its local reachability density (lrd), which is the inverse of the average \n",
    "    reachability distance of the data point to its k-NN. It quantifies how dense the local neighborhood of the data\n",
    "    point is. The formula for lrd is often given as:\n",
    "\n",
    "            lrd(A) = 1 / (Σ reachability_distance(A, B) for all B in k-NN of A)\n",
    "\n",
    "5.Compute Local Outlier Factor (LOF):\n",
    "    \n",
    "    ~Finally, LOF computes the local outlier factor for each data point. The LOF of a data point A is the average \n",
    "    ratio of the lrd of A to the lrd of its k-NN. This ratio indicates how much denser or sparser A's neighborhood \n",
    "    is compared to its neighbors. A high LOF indicates that the point is an outlier, as it has a significantly\n",
    "    different local density compared to its neighbors.\n",
    "\n",
    "            LOF(A) = (Σ lrd(B) for all B in k-NN of A) / (k * lrd(A))\n",
    "\n",
    "6.Anomaly Score: The LOF value computed in step 5 serves as the anomaly score for each data point. Data points with \n",
    "high LOF values are considered anomalies, as they have local densities significantly different from their neighbors,\n",
    "indicating that they are in sparser or denser regions.\n",
    "\n",
    "In summary, the LOF algorithm assesses the local density of data points by comparing their reachability distances to\n",
    "those of their neighbors. The ratio of a data point's local reachability density to the average local reachability\n",
    "density of its neighbors results in the LOF score, which is used to identify anomalies in the dataset. High LOF values\n",
    "correspond to data points that exhibit unusual local density patterns compared to their neighbors, making them\n",
    "potential anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3c69b-bf2d-401e-afa8-28d219931286",
   "metadata": {},
   "source": [
    "## Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78344b5d-2f55-4ca8-930b-636c54f9dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Isolation Forest algorithm is an anomaly detection method that operates on the principle of isolating anomalies \n",
    "in a dataset by constructing random decision trees. It is a popular and efficient algorithm for detecting anomalies,\n",
    "especially in high-dimensional data. The main parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1.n_estimators (or n_trees):\n",
    "\n",
    "    ~Definition: The number of isolation trees to be created in the forest.\n",
    "    ~Default Value: Typically set to 100, but it can vary depending on the dataset and problem.\n",
    "    \n",
    "2.max_samples:\n",
    "\n",
    "    ~Definition: The number of data points to be randomly sampled to create each isolation tree. It controls the size\n",
    "    of the subsets used to construct individual trees.\n",
    "    ~Default Value: Often set to \"auto,\" which means that it is set to the size of the input data. Setting it to a\n",
    "    smaller value can speed up the algorithm but may reduce its effectiveness in capturing complex anomalies.\n",
    "    \n",
    "3.contamination:\n",
    "\n",
    "    ~Definition: The estimated proportion of anomalies in the dataset. It is used to set a threshold for identifying \n",
    "    anomalies based on the average path length in the trees.\n",
    "    ~Default Value: Typically set to \"auto,\" which means it is estimated from the data. Alternatively, you can provide\n",
    "    a specific value representing the expected proportion of anomalies in the dataset.\n",
    "    \n",
    "4.max_features:\n",
    "\n",
    "    ~Definition: The maximum number of features (variables) to consider when splitting nodes in the decision trees. \n",
    "    It can be set as an integer or a float.\n",
    "    ~Default Value: Often set to 1.0 (consider all features). Reducing this value can lead to more randomization in\n",
    "    tree construction, potentially improving the algorithm's ability to capture anomalies.\n",
    "    \n",
    "5.random_state:\n",
    "\n",
    "    ~Definition: A seed or random number generator state that ensures reproducibility of the results. Setting this \n",
    "    parameter to a specific value ensures that the same random trees are generated each time the algorithm is run\n",
    "    with the same data.\n",
    "    ~Default Value: Typically set to None.\n",
    "    \n",
    "These are the primary parameters that you can tune when using the Isolation Forest algorithm. The choice of parameter \n",
    "values can significantly impact the algorithm's performance, so it's often necessary to perform hyperparameter tuning \n",
    "to find the optimal combination for a specific dataset and problem.\n",
    "\n",
    "In practice, you can use techniques such as cross-validation or grid search to select the best parameter values for\n",
    "your anomaly detection task. Adjusting parameters like \"n_estimators,\" \"max_samples,\" and \"max_features\" can influence\n",
    "the trade-off between computational efficiency and the algorithm's ability to detect anomalies accurately. The \n",
    "\"contamination\" parameter is particularly important, as it determines the anomaly detection threshold, affecting the\n",
    "balance between false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991d980-e21b-436a-9f33-caf1cfdd7ba3",
   "metadata": {},
   "source": [
    "## Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206767f-3886-4e21-b21f-b086ddbafa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "To calculate the anomaly score for a data point using the k-nearest neighbors (KNN) algorithm with K=10, you need to\n",
    "consider the number of neighbors of the same class within a specified radius. In your case, you want to calculate the\n",
    "anomaly score for a data point that has only 2 neighbors of the same class within a radius of 0.5.\n",
    "\n",
    "The anomaly score can be calculated as follows:\n",
    "\n",
    "1.If the data point has 2 neighbors of the same class within the radius, this means that it has 2 inliers (normal\n",
    "points) within the specified radius.\n",
    "\n",
    "2.Since K=10, we are considering the 10 nearest neighbors in total.\n",
    "\n",
    "3.The anomaly score is often calculated as the ratio of inliers to the total number of neighbors within the radius. \n",
    "In this case, it's 2 (inliers) divided by 10 (total neighbors).\n",
    "\n",
    "4.To get a more interpretable anomaly score, you can multiply this ratio by 100 to express it as a percentage:\n",
    "\n",
    "            Anomaly Score = (2 / 10) * 100 = 20%\n",
    "\n",
    "So, the anomaly score for the data point is 20%. This indicates that the data point is relatively close to its\n",
    "neighbors of the same class within the specified radius, which suggests it is less anomalous. An anomaly score of\n",
    "20% means it is closer to being normal in the context of the KNN algorithm with K=10 and the given criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5874504-30bb-46f3-a8e6-e9d1e04d8512",
   "metadata": {},
   "source": [
    "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908f5be-a412-4e89-8466-b942041b159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is determined by its average path length in\n",
    "a collection of decision trees. Lower average path lengths indicate that a data point is easier to isolate, suggesting\n",
    "it may be an anomaly. Comparing a data point's average path length to the average path length of the trees helps\n",
    "determine its anomaly score.\n",
    "\n",
    "In your scenario, you have:\n",
    "\n",
    "    ~100 trees in your Isolation Forest.\n",
    "    ~A dataset with 3000 data points.\n",
    "    ~A data point with an average path length of 5.0 compared to the average path length of the trees.\n",
    "    \n",
    "To calculate the anomaly score for this data point, you can use the following steps:\n",
    "\n",
    "1.Calculate the average path length of the trees:\n",
    "\n",
    "    ~The average path length for the entire dataset is typically used as a reference point. This represents the\n",
    "    average path length for normal data points. Let's assume this average path length for the trees is A.\n",
    "\n",
    "2.Calculate the anomaly score for the data point:\n",
    "\n",
    "    ~The data point has an average path length of 5.0, which we'll call B.\n",
    "    ~Anomaly Score = 2^(-B/A)\n",
    "    \n",
    "In this formula:\n",
    "\n",
    "    ~A is the average path length for the entire dataset (average path length of the trees).\n",
    "    ~B is the average path length for the specific data point.\n",
    "    \n",
    "Plug in the values:\n",
    "\n",
    "    ~If A is the average path length of the trees (which you would calculate from your forest), and\n",
    "    ~B is 5.0 (the average path length of the data point),\n",
    "    \n",
    "        Anomaly Score = 2^(-5.0/A)\n",
    "\n",
    "This formula calculates the anomaly score based on the relative average path length of the data point compared to the\n",
    "average path length of the trees. A lower score indicates a higher likelihood of being an anomaly. Please note that\n",
    "you'll need to calculate the actual average path length of the trees in your Isolation Forest model to obtain the \n",
    "precise anomaly score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
